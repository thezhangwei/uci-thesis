\chapter{Related Work}
\label{chp:ch8-related-work}

Python, similar to other popular dynamic languages, is originally implemented as an interpreter written in C, namely CPython~\cite{python}.
Google later on developed unladen swallow, an optimizing branch of CPython, attempting to improve its performance by adopting JIT compilation techniques.
As the existing virtual machines built for traditional statically typed programming languages, such as JVM and Microsoft CLR~\cite{Kennedy+2001,Gough+2001,kilgore2002open}, mature and gain more popularity, they start to become multi-tenant.
This movement drove the appearance of multiple hosted Python implementations:
IronPython is a Python implementation that runs on Microsoft's CLR architecture~\cite{hugunin2004ironpython}.
Jython is a Python implementation written in Java and hosted on the JVM~\cite{jython}.
As a more recent work, PyPy\cite{Rigo2006,bolz.etal09,Bolz+2011,Bolz+alloc+2011,Bolz+2013,bolz.etal+15,Ardo+2012,Schneider+2012} is a fast and highly compliant implementation of the Python programming language.
PyPy uses a novel meta-tracing JIT compilation approach to speedup execution of Python programs.
Their approach separates the hosted Python bytecode interpreter from the underlying tracing compiler, and enables PyPy as a framework for other language implementations.

\section{Hosted Interpreters for Dynamic Languages}

Implementing a complete high-performance VM for each language is not a scalable approach.
Therefore, there are several projects that seek to build a framework supporting multiple dynamic languages.

Our system builds on the publicly available Truffle framework of Oracle Labs~\cite{Wurthinger+12}.
Following previously successful research in type specialization of bytecode interpreters (cf.~\cite{Brunthaler2010inca,Brunthaler2010quickening,williams.etal+10}), W\"{u}rthinger et al. apply the concept of quickening to AST nodes, hence the term ``node rewriting.''
The primary benefit of doing this on the AST level is that rewriting operations can be much more general, since they are not restricted to the rigidity superimposed by bytecode representation.
In this thesis we have addressed some of the trade-offs between interpreting on ASTs or bytecode interpreters.
The Truffle framework provides a code generation system for generating type-specialized derivative nodes, as well as a generic type specialization mechanism.
W\"{u}rthinger et al. conjecture that such a high-level optimized AST representation is ideally suitable for just-in-time compilation.
In a more recent advance~\cite{Wurthinger+13}, they explore the suitability of partial evaluation followed by just-in-time compilation, which raises expectations for high speedups.
Based on our experiences obtained by implementing our full-fledged prototype, we concur with these expectations.

Our implementation differs from their work primarily in exploring and extending the applicability and performance to Python 3.
Not only do we measure the type specialization potential and compare implementation effort, but we also explore necessary optimizations to obtain high performance on a stock Java virtual machine.
By leveraging the recently explored partial evaluation plus JIT compiler combination,
we are able to gain substantial speedups on top of our platform---requiring only little extra implementation effort.

Similar to Truffle, the PyPy project~\cite{Rigo2006} also aims to provide an easy to use framework for dynamic language implementers.
There are several similarities, such as using RPython instead of C/C++ to implement interpreters.
The differences are more of a subtle nature.
First, PyPy mostly  focuses on bytecode interpreters instead of AST interpreters (though they are supported).
Second, PyPy uses a trace-based compilation approach relying on the bytecode interpreter~\cite{bolz.etal09}.
In theory, however, PyPy is not bound to the bytecode interpreter and could very well duplicate the Truffle approach.
Furthermore, trace-based compilation~\cite{gal.etal+09,chang.etal+09,bebenita.etal+10,bebenita.etal+09,wimmer.etal+09} is somewhat similar to partial evaluation followed by just-in-time compilation.
Third, PyPy does not target an existing virtual machine, i.e., it offers more degrees of freedom to language implementers.

For our project, these differences are, however, not relevant.
While the PyPy project successfully supports multiple languages, it does not offer a framework to generate interpreter instructions plus type specialization.
On the other hand, for some languages, the Java virtual machine object model is not going to be a good fit, where the custom virtual machine approach from PyPy is going to be more attractive.

Yet another way to ``host'' a dynamic languages was presented by Ishizaki et al.~\cite{Ishizaki.etal+12}.
They describe an approach to optimize dynamic languages by repurposing an existing JIT compiler for a statically typed programming language.
They reuse the IBM J9 Java VM, and extend its JIT compiler to optimize a dynamic language.
They experiment their approach on Python 2, and present optimizations that are specifically beneficial for dynamic languages.

The primary difference between the repurposed virtual machine approach and our implementation --- probably generalizes to all implementations based on Truffle --- is that we have a higher-level view of optimizations.
Since we are not bound by the CPython bytecode representation, the AST interpreter has more potential to perform inlining.
Similar to PyPy, the repurposed VM approach does not make implementation of new languages easier, in the sense that they do not provide a code generator.

Type feedback goes back to the successful optimization efforts of the \textsc{Self} programming language~\cite{holzle.ungar+94,holzle+94}, extending previously successful results for implementing the Smalltalk-80 system with inline caches and just-in-time compilation (along with pointing out the importance of deoptimization)~\cite{Deutsch1984}.
We refer the interested reader to a concise survey covering the state-of-the-art until 2003~\cite{aycock+03}.
Using traditional assembly inline caching to gather type feedback and subsequently performing speculative optimizations is the key to efficient just-in-time compiling highly dynamic programming languages~\cite{chang.etal+11}.
Since 2010, bytecode interpreters benefit from simple and efficient inline caching, resulting in substantial speedups~\cite{Brunthaler2010inca,williams.etal+10}.

Our work does not require handling assembly code, but leverages efficient Java just-in-time compilers~\cite{kotzmann.etal+08,paleczny.etal+01,wimmer.franz+10}, which directly build on the earlier
\textsc{Self} and Smalltalk results. Similarly, by operating on AST nodes rather than a bytecode representation, we avoid the need to define this virtual instruction set, and sidestep the implementation effort required to compile to this set of bytecode.

\section{Truffle Languages}

The approach of using Truffle/Graal to optimize implementations of dynamic languages via specialization and other dynamic optimizations on an AST-based interpreter has become a hot topic in the language runtime research community.
We have seen recent works on using Truffle to optimize dynamic programming languages other than Python as well as other aspects of language runtimes that are traditionally regarded as difficult to optimize such as debugging and native library interfacing.

Chris Seaton et al.~\cite{seaton2014debugging} demonstrated their use of Truffle to provide low overhead debugging in their Truffle-based Ruby implementation.
Their work now has become part of the JRuby project.
Grimmer et al. proposed \emph{GNFI}~\cite{Grimmer+2013} a new native interface for the JVM that uses Graal to generate the native call stub dynamically at runtime.
Grimmer et al. later on extended their work onto TruffleJS~\cite{Grimmer+2014}, a Truffle-based JavaScript implementation written in Java.
They described how a JavaScript application running on TruffleJS can access C data structure efficiently with the help of TruffleC~\cite{Grimmer+2014TruffleC}, a C interpreter built using Truffle.
W\"{o}\ss et al.~\cite{WoB+2014} described Truffle OSM, a high-performance object storage model for the Truffle framework.
This work was originally designed for TruffleJS and later on included as part of the Truffle framework.
Truffle OSM is similar to the fixed object storage described in this thesis.
The flexible object storage implementation in ZipPy is however a generalization of Truffle OSM that takes advantage of the existence of class in Python.

In a more recent work, Grimmer et al.~\cite{grimmer2015dynamically, Grimmer+2014interoperability} demonstrated the potential of language interoperability on top of Truffle by hosting multiple Truffle language runtimes on the same JVM instance.
They introduced an interface for shareable objects that allows different language runtimes to exchange data with converting objects from one language to the other.
Marr et al.~\cite{marrzero} showed that the overhead of reflective operations and metaobject protocols can be eliminated by using polymorphic cache inspired \emph{dispatch chains}.
They demonstrated their work in the context of self-optimizing interpreters running on top of Truffle.

\section{Generators and Coroutines}

% original generator idea
Murer et al.~\cite{Murer1996} presented the design of Sather iterators derived from the iterators in CLU~\cite{Liskov1977}.
Sather iterators encapsulate their execution states and may ``yield'' or ``quit'' to the main program.
This design inspired the design of generators in Python.

% efficient coroutine for JVM
Stadler et al.~\cite{Stadler2010} presented a coroutine implementation for the JVMs that can efficiently handle coroutine stacks by letting a large number of coroutines share a fixed number of stacks.
Our generator solution does not rely on coroutine stacks and does not require modifications to the host language.

% Ruby's block inlining
In Ruby~\cite{ruby}, methods may receive a code block from the caller.
The method may invoke the code block using ``yield'' and pass values into the code block.
Ruby uses this block parameter to implement iterators.
An iterator method expects a code block from the caller and ``yields'' a series of values to the block.
To optimize the iterator method, an efficient Ruby implementation can inline the iterator method to the caller and further inline the call to the code block.
This optimization combines the iterator method and the code block in the same context, and resembles the generator peeling transformation.
However, iterator methods in Ruby are different from generator functions in Python.
They do not perform generator suspends and resumes.
Generator peeling employs additional program analysis and high level transformations, hence is more sophisticated than straight forward call inlining.

Both CLU~\cite{Atkinson1978CLU} and JMatch~\cite{Liu:2006JMatch} have both implemented a frame optimization for the iterator feature in their languages.
To avoid heap allocation, their optimizations allocate iterator frames on the machine stack.
When an iterator \emph{yields} back to the caller, its frame remains intact on the stack.
When resuming, the optimized program switches from the caller frame to the existing iterator frame by restoring the frame pointer, and continues execution.
Their approaches, require additional frame pointer manipulation and saving the program pointer of the iterator to keep track of the correct program location.
Generator peeling, on the other hand, is an interpretation level specialization, and does not introduce low-level modifications to the compiler to generate special machine code for generators.
It allows compiler optimizations to map the caller frame and the generator frame accesses to the same machine stack frame, and does not require saving the generator function program pointer to resume execution.
Therefore it is more efficient.

Watt~\cite{watt2006technique} describes an inlining based technique that optimizes control-based iterators in Aldor, a statically typed language.
His approach requires multiple extra steps that iteratively optimize the data structures and the control flows after the initial inlining.
Generator peeling transforms the guest program AST in a single step before the compilation starts.
It simplifies the workload for the underlying compiler and enables more optimizations.
