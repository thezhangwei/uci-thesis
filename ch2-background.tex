\chapter{Background}
\label{chp:ch2-background}

\section{Virtual Machines}
\label{sec:ch2-virtual-machines}

In this thesis we refer process virtual machines or application virtual machines simply as VMs.
They usually supports a single process running in a hosted operating system.
A VM provides a high-level abstraction composing a high-level programming language compared to the traditional low-level system programming languages.
This type of VM become popular since the wide adoption of the Java virtual machine which implements the Java programming language.
Microsoft's common language runtime is another example.
V8~\cite{v8}, a JavaScript implementation developed by Google, is a more recent popular language VM.

Virtual machines execute the hosted program in various fashions.
The VM can execute the hosted code using an interpreter, a just-in-time (JIT) compiler or a combination of both.
First the VM parses the targeted program from source code to a form of intermediate representation (IR).
The IR consists of series of ``instructions'' with each ``instruction'' representing exactly one functional operation, e.g., an arithmetic additional operation.
The interpreter then executes the program by translating the IR into actions one piece at a time.
For instance the interpreter interprets the IR instruction representing an additional operation by performing the actual addition.
The JIT compiler on the other hand execute the program by translating the IR into machine code and then redirect execution to the compiled machine code.

Interpreters often subject to suboptimal performance.
This is partially caused to the cost of by having to process each instruction before executing it.
Whereas a JIT compiler performs the translation prior to the execution of the program and avoid the overhead of processing repetitively executed instructions.
In addition, JIT compilers often implement more aggressive optimizations that potentially skip the execution of some portions of the program altogether.
However, the time the JIT compiler spend in compiling the program does not directly contribute to the execution of the target program.
This delay in compilation makes JIT compilers less ideal for programs that requires fast response.
A VM execution strategy called ``mixed mode'' addresses the disadvantages of both options by combining an interpreter and JIT compiler.
The VM initially executes the target program using the interpreter for fast response.
As the program becomes ``hot'', the VM switches the its execution by using the JIT compiler for fast execution.
The VM switches the execution strategy at the granularity of a compilation unit or a method as defined in the target language.
The initial use of interpreter also helps to collect more runtime information regarding the target program that can benefit the optimizations later on performed by the JIT compiler.

\section{Interpreters}

Interpreters directly executes a target program without previously compiling them into machine code~\cite{debaere.campenhout+90, kogge+82}.
Most interpreters operate on two form of IRs: abstract syntax tree (AST)~\cite{jones2003abstract} and bytecode.
ASTs are the most simple and natural way to represent computer programs.
It is straight forward to produce from the source program and easy to manipulate due to the nature of a tree data structure.
However, interpreting an AST requires an expensive tree traversal step at each node.
The overhead caused by the tree traversal is the main drawback on the performance of an AST interpreter.

Bytecode is a more compact form of IR consisting of a sequence of virtual instructions or opcode.
Each virtual instruction is a number of bytes encoding the detail of a program operation.
The interpretation of bytecode is more efficient than AST since it does not require traversing a tree data structure.
In addition, the VM can encode the result of semantics analysis of types and scopes in the bytecode, therefore allows better performance.
On the other hand, bytecode is more rigid than ASTs.
The format of a bytecode instruction set is predetermined.
It is complicated to apply transformations at runtime because of the compact data representation of bytecode.

Interpreters are in general simpler to implement and less expensive to maintain.
They are also portable.
Since the implementation of an interpreter is not platform dependent, it runs on all the platform where the language used to implement the interpreter supports.
As a result, the development cycle for a interpreter is considerably shorter than that of a compiler.
It is more affordable for a language implementer to make changes to their language when implemented as an interpreter.

\section{Just-In-Time Compilers}

JIT compilers, in contrast to traditional compilers that perform compilation ahead of time (AOT), translate target program into machine code just prior to their execution at runtime.
The main advantage of a JIT compiler is that is can make use of information only available at runtime to produce better code.
For instance, a JIT compiler can make use of Intel's SSE2~\cite{klimovitski2001using} instructions to accelerate floating point operations if it detects that the CPU support them.
An AOT compiler on the other hand cannot assume the availability of SSE2 instructions at runtime, therefore must conservatively compile the target program without the use of SSE2.

There is a wide spectrum between the simplest JIT compilers and the more advanced ones.
The most simple way to implement a JIT compiler is to use a compilation template.
A template compiler translates target program from the input IR to machine code using a straight forward translation.
That is always translating an interpreter instruction to a predetermined sequence of machine code.
The compiler essentially stitches all the translated templates into a single piece of machine code and executes it.
More advanced JIT compilers implement more sophisticated internal intermediate representations and advanced optimization techniques.
For example, Graal~\cite{duboscq2013graal}, a Java JIT compiler developed at Oracle Labs, uses a graph-based IR that represents a Java program as a sea of graph nodes.
The IR models both control-flow and data-flow dependencies between nodes that allows variety of transformations performed by the compiler.
Graal also implements advanced optimizations such as partial escape analysis and scalar replacement for Java~\cite{stadler2014partial}.

One of the most important trade off of using a JIT compiler is compilation time.
The longer the compiler spend in compilation the longer it delays the actual execution of the program, resulting a longer overall execution time.
However, allocating more compilation time allows the JIT compiler to perform more expensive optimizations to produce more efficient code for long running programs.
Modern VMs use a tier compilation strategy that involves more than one JIT compilers to execute the target program.
The VM first uses a less powerful compiler to produce machine code in a short period of time, and switches to a more powerful compiler for ``hotter'' or longer running methods.
This combination allows optimal performance for both short and long running programs.

\section{Type Specialization for Dynamic Languages}

Dynamic languages like Python allows symbols or variables to be associated with values of \emph{any} type.
The type information of a variable is typically unknown before execution.
This lack of type information is the main challenge of compiling and optimizing programs written in dynamic languages.
Prior to the execution of an operation, the type of its operands must be discovered.
This information is necessary for the VM to perform the actual computation.
For instance, for an addition operation in Python, if both of the operands are integers, an integer addition occurs.
If both operands are strings, a string concatenation should be performed.
This overhead of resolving type information for each operation significantly increases the cost of executing programs written in dynamic languages.

However, value types at a particular program location tend to be stable.
Deustch et al.~\cite{Deutsch1984} observed this ``type stable'' effect in their work in Smalltalk.
That is once a variable is assigned to an integer, it is most likely to stay as an integer in successive executions.
Richards et al.~\cite{Richards+2010} presented that in JavaScript programs $80\%$ of all the call sites has only one type.
Therefore, to speedup dynamic languages, the VMs need to use JIT compilers to produce better code by exploiting type stabilities.

Type specialization is a technique implemented in modern dynamic language VMs that eliminates the overhead of type checking at runtime.
Type specialization speculatively assumes that the operands' type of an operation stays stable, hence, perform the operation as if it is typed.
For example, if an addition operation was previously resolved as integer addition, type specialization performs all subsequent the executions as integer addition until its operands are not integers anymore.
Since we only need perform the type check against integers, the JIT compiler can optimize these simple checks.

Chambers and Ungar~\cite{Chambers+1989} demonstrated method customizations in \textsc{Self}.
Their solution generates a customized machine code version using the JIT compiler for each typed signature of a method.
Deustch et al.~\cite{Deutsch1984} introduced inline caching in Smalltalk.
Inline caching caches the type information for the initial execution of an operation.
It assumes type stability for successive executions to alleviate the type checking cost.
Holzle et al.~\cite{Holzle+1991} introduced polymorphic inline caching (PIC).
PIC extends inline caching to cache multiple operand types for which the type checking is still cheaper than a full lookup.

Brunthaler~\cite{Brunthaler2010inca} applies quickening to bytecode interpreters, and demonstrated substantial speedup in his optimized Python interpreter.
Quickening performs type specialization by rewriting the program input bytecode sequence to cache type information.
W\"{u}rthinger~\cite{Wurthinger+12} proposed self-optimizing AST interpreters that use profiling and node rewriting to accelerate its execution.
A node rewrite attempts to replace the existing AST node with a type-specialized version optimized for the cached operand types.

